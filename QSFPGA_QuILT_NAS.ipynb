{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM0THTS2MENKFcds0sR6Nt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbabulik/QSFPGA/blob/main/QSFPGA_QuILT_NAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al-EHF6hTSan",
        "outputId": "8e015169-6e17-437c-bf73-13aa1cce8474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- QSFPGA SYSTEM STARTUP ---\n",
            "Compute Device: cpu\n",
            "Downloading Mathematics Dataset...\n",
            "Extracting...\n",
            "Loading Data...\n",
            "Training Samples: 666466\n",
            "Test Samples:     200\n",
            "\n",
            "--- PHASE 1: COLLECTING FITNESS LANDSCAPE (AUTONOMOUS) ---\n",
            "[1/16] Assessing Arch-0000... Cost: -1.00\n",
            "[2/16] Assessing Arch-0001... Cost: -1.00\n",
            "[3/16] Assessing Arch-0010... Cost: -1.00\n",
            "[4/16] Assessing Arch-0011... Cost: -1.00\n",
            "[5/16] Assessing Arch-0100... Cost: -1.00\n",
            "[6/16] Assessing Arch-0101... Cost: -1.00\n",
            "[7/16] Assessing Arch-0110... Cost: -1.00\n",
            "[8/16] Assessing Arch-0111... Cost: -1.00\n",
            "[9/16] Assessing Arch-1000... Cost: -1.00\n",
            "[10/16] Assessing Arch-1001... Cost: -1.00\n",
            "[11/16] Assessing Arch-1010... Cost: -1.00\n",
            "[12/16] Assessing Arch-1011... Cost: -1.00\n",
            "[13/16] Assessing Arch-1100... Cost: -1.00\n",
            "[14/16] Assessing Arch-1101... Cost: -1.00\n",
            "[15/16] Assessing Arch-1110... Cost: -1.00\n",
            "[16/16] Assessing Arch-1111... Cost: -1.00\n",
            "\n",
            "--- PHASE 2: QuILT OPTIMIZATION ---\n",
            "Notice: Fitness landscape is flat (models need more training time). Injecting microspectrum noise for demo.\n",
            "Optimizing Quantum State...\n",
            "VQE Epoch 0: System Energy = -0.9909\n",
            "VQE Epoch 20: System Energy = -1.0095\n",
            "VQE Epoch 40: System Energy = -1.0103\n",
            "VQE Epoch 60: System Energy = -1.0104\n",
            "VQE Epoch 80: System Energy = -1.0104\n",
            "VQE Epoch 100: System Energy = -1.0104\n",
            "\n",
            "==================================================\n",
            "  OPTIMAL ARCHITECTURE FOUND: Arch-0111\n",
            "==================================================\n",
            "Quantum Probability: 0.9999\n",
            "Configuration Specs:\n",
            "   Layers:     4\n",
            "   Embed Dim:  64\n",
            "   Heads:      4\n",
            "   Block Size: 128\n",
            "   LR:         0.0005\n",
            "--------------------------------------------------\n",
            "\n",
            "==================================================\n",
            "  OPTIMAL ARCHITECTURE FOUND: Arch-0111\n",
            "==================================================\n",
            "Quantum Probability: 0.9999\n",
            "Configuration Specs:\n",
            "   Layers:     4\n",
            "   Embed Dim:  64\n",
            "   Heads:      4\n",
            "   Block Size: 128\n",
            "   LR:         0.0005\n",
            "--------------------------------------------------\n",
            "Ready for Full Scale Training on chosen architecture.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 0: Environment & Data Setup (Automated)\n",
        "# ==============================================================================\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"--- QSFPGA SYSTEM STARTUP ---\")\n",
        "print(f\"Compute Device: {device}\")\n",
        "\n",
        "# 1. Download Data if missing\n",
        "data_dir = \"mathematics_dataset-v1.0\"\n",
        "if not os.path.exists(data_dir):\n",
        "    print(\"Downloading Mathematics Dataset...\")\n",
        "    subprocess.run([\"curl\", \"-s\", \"-O\", \"https://storage.googleapis.com/mathematics-dataset/mathematics_dataset-v1.0.tar.gz\"])\n",
        "    print(\"Extracting...\")\n",
        "    subprocess.run([\"tar\", \"-xzf\", \"mathematics_dataset-v1.0.tar.gz\"])\n",
        "else:\n",
        "    print(\"Dataset already present.\")\n",
        "\n",
        "# 2. Load Specific Module (Arithmetic)\n",
        "TRAIN_FILE_PATH = f\"{data_dir}/train-easy/arithmetic__add_or_sub.txt\"\n",
        "\n",
        "def load_data(filepath, holdout=200):\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"Error: File {filepath} not found. Check download.\"); sys.exit()\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Clean trailing newlines\n",
        "    if len(lines) % 2 != 0: lines = lines[:-1]\n",
        "\n",
        "    pairs = []\n",
        "    for i in range(0, len(lines), 2):\n",
        "        q = lines[i].strip()\n",
        "        a = lines[i+1].strip()\n",
        "        pairs.append((q, a))\n",
        "\n",
        "    # Split\n",
        "    test_pairs = pairs[-holdout:]\n",
        "    train_pairs = pairs[:-holdout]\n",
        "\n",
        "    # Flatten train for tokenizer\n",
        "    text_data = \"\"\n",
        "    for q, a in train_pairs:\n",
        "        text_data += f\"{q}\\n{a}\\n\"\n",
        "\n",
        "    return text_data, test_pairs\n",
        "\n",
        "print(\"Loading Data...\")\n",
        "train_text, test_qa_pairs = load_data(TRAIN_FILE_PATH)\n",
        "print(f\"Training Samples: {len(train_text.splitlines())//2}\")\n",
        "print(f\"Test Samples:     {len(test_qa_pairs)}\")\n",
        "\n",
        "# 3. Tokenizer\n",
        "chars = sorted(list(set(train_text)))\n",
        "vocab_size = len(chars)\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Prepare Tensor Data\n",
        "data_tensor = torch.tensor(encode(train_text), dtype=torch.long)\n",
        "n_split = int(0.9*len(data_tensor))\n",
        "train_data = data_tensor[:n_split]\n",
        "val_data = data_tensor[n_split:]\n",
        "\n",
        "def get_batch(split, block_size, batch_size):\n",
        "    d = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(d) - block_size, (batch_size,))\n",
        "    x = torch.stack([d[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([d[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 1: The picoTransformer Model\n",
        "# ==============================================================================\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config['n_embd'] % config['n_head'] == 0\n",
        "        self.c_attn = nn.Linear(config['n_embd'], 3 * config['n_embd'])\n",
        "        self.c_proj = nn.Linear(config['n_embd'], config['n_embd'])\n",
        "        self.attn_dropout = nn.Dropout(config['dropout'])\n",
        "        self.resid_dropout = nn.Dropout(config['dropout'])\n",
        "        self.n_head = config['n_head']\n",
        "        self.n_embd = config['n_embd']\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config['block_size'], config['block_size']))\n",
        "                                     .view(1, 1, config['block_size'], config['block_size']))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config['n_embd'])\n",
        "        self.sa = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config['n_embd'])\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config['n_embd'], 4 * config['n_embd']),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config['n_embd'], config['n_embd']),\n",
        "            nn.Dropout(config['dropout']),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class PicoTransformer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config['vocab_size'], config['n_embd']),\n",
        "            wpe = nn.Embedding(config['block_size'], config['n_embd']),\n",
        "            drop = nn.Dropout(config['dropout']),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config['n_layer'])]),\n",
        "            ln_f = nn.LayerNorm(config['n_embd']),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config['n_embd'], config['vocab_size'], bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
        "\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, end_token_id=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config['block_size'] else idx[:, -self.config['block_size']:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            if end_token_id and idx_next.item() == end_token_id:\n",
        "                break\n",
        "        return idx\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 2: Automated Architecture Search (Neural Architecture Search)\n",
        "# ==============================================================================\n",
        "\n",
        "def automated_judge(question, model_answer, correct_answer):\n",
        "    \"\"\"\n",
        "    Replaces the Human.\n",
        "    Checks if the Model Answer numerically matches the Correct Answer.\n",
        "    \"\"\"\n",
        "    # 1. Clean strings\n",
        "    ma_clean = model_answer.strip().replace('\\n', '')\n",
        "    ca_clean = correct_answer.strip().replace('\\n', '')\n",
        "\n",
        "    # 2. Exact string match\n",
        "    if ma_clean == ca_clean:\n",
        "        return 10.0\n",
        "\n",
        "    # 3. Numerical Fallback (in case of extra spaces)\n",
        "    try:\n",
        "        if abs(float(ma_clean) - float(ca_clean)) < 1e-4:\n",
        "            return 10.0\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # 4. Penalty for wrong answer\n",
        "    return 1.0\n",
        "\n",
        "def evaluate_fitness_autonomous(config, iterations=500):\n",
        "    \"\"\"Trains a model briefly and auto-scores it.\"\"\"\n",
        "\n",
        "    # Init Model\n",
        "    model = PicoTransformer(config).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "\n",
        "    # Quick Train\n",
        "    model.train()\n",
        "    for _ in range(iterations):\n",
        "        xb, yb = get_batch('train', config['block_size'], config['batch_size'])\n",
        "        _, loss = model(xb, yb)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluation (Test on 5 holdout problems)\n",
        "    model.eval()\n",
        "    total_score = 0\n",
        "    num_tests = 5\n",
        "    end_token = stoi.get('\\n', None) # Stop at newline\n",
        "\n",
        "    # Context window logic\n",
        "    for _ in range(num_tests):\n",
        "        q, a = random.choice(test_qa_pairs)\n",
        "        # Format: \"Question\\n\" -> Model expects to generate Answer\n",
        "        prompt = f\"{q}\\n\"\n",
        "        context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "        try:\n",
        "            out_ids = model.generate(context, max_new_tokens=15, end_token_id=end_token)[0].tolist()\n",
        "            generated = decode(out_ids[len(encode(prompt)):])\n",
        "            score = automated_judge(q, generated, a)\n",
        "        except Exception as e:\n",
        "            score = 0.0 # Crash penalty\n",
        "\n",
        "        total_score += score\n",
        "\n",
        "    avg_score = total_score / num_tests\n",
        "    # Inverse Cost: High Score (10) -> Low Cost (-10)\n",
        "    return -avg_score\n",
        "\n",
        "# Define Search Space (4 Qubits = 16 States)\n",
        "n_qubits_nas = 4\n",
        "bitstrings = [np.binary_repr(i, width=n_qubits_nas) for i in range(2**n_qubits_nas)]\n",
        "\n",
        "def decode_arch(b):\n",
        "    # Mapping bits to hyperparameters\n",
        "    # Bits 0-1: Size\n",
        "    size_map = {\n",
        "        '00': {'n_layer': 2, 'n_embd': 64, 'n_head': 4},    # Tiny\n",
        "        '01': {'n_layer': 4, 'n_embd': 64, 'n_head': 4},    # Small\n",
        "        '10': {'n_layer': 4, 'n_embd': 128, 'n_head': 8},   # Medium\n",
        "        '11': {'n_layer': 6, 'n_embd': 128, 'n_head': 8}    # Large\n",
        "    }\n",
        "    cfg = size_map[b[0:2]].copy()\n",
        "\n",
        "    # Bit 2: Block Size (Context)\n",
        "    cfg['block_size'] = 64 if b[2] == '0' else 128\n",
        "\n",
        "    # Bit 3: Learning Rate\n",
        "    cfg['lr'] = 1e-3 if b[3] == '0' else 5e-4\n",
        "\n",
        "    # Fixed params\n",
        "    cfg['vocab_size'] = vocab_size\n",
        "    cfg['batch_size'] = 32\n",
        "    cfg['dropout'] = 0.1\n",
        "    cfg['name'] = f\"Arch-{b}\"\n",
        "    return cfg\n",
        "\n",
        "configs = [decode_arch(b) for b in bitstrings]\n",
        "\n",
        "print(\"\\n--- PHASE 1: COLLECTING FITNESS LANDSCAPE (AUTONOMOUS) ---\")\n",
        "costs = []\n",
        "for i, cfg in enumerate(configs):\n",
        "    print(f\"[{i+1}/{len(configs)}] Assessing {cfg['name']}...\", end=\"\")\n",
        "    cost = evaluate_fitness_autonomous(cfg, iterations=200) # Fast check\n",
        "    costs.append(cost)\n",
        "    print(f\" Cost: {cost:.2f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 3: QuILT (Quantum Optimization) - CORRECTED\n",
        "# ==============================================================================\n",
        "\n",
        "class VQESelector(nn.Module):\n",
        "    def __init__(self, n_qubits, depth=2):\n",
        "        super().__init__()\n",
        "        self.n_qubits = n_qubits\n",
        "        self.depth = depth\n",
        "        # Learnable Rotation Angles\n",
        "        self.theta = nn.Parameter(torch.rand(depth, n_qubits, 2) * 2 * math.pi)\n",
        "\n",
        "    def forward(self):\n",
        "        # Digital Twin of the FPGA Hardware Logic\n",
        "        psi = torch.zeros(2**self.n_qubits, dtype=torch.cfloat, device=device)\n",
        "        psi[0] = 1.0\n",
        "\n",
        "        # Identity for tensor expansion\n",
        "        I = torch.eye(2, device=device, dtype=torch.cfloat)\n",
        "\n",
        "        for d in range(self.depth):\n",
        "            for q in range(self.n_qubits):\n",
        "                # --- APPLY RY ---\n",
        "                ang_y = self.theta[d, q, 0]\n",
        "                cy = torch.cos(ang_y / 2)\n",
        "                sy = torch.sin(ang_y / 2)\n",
        "\n",
        "                # FIX: Use torch.stack instead of torch.tensor to keep gradients alive\n",
        "                row0 = torch.stack([cy, -sy])\n",
        "                row1 = torch.stack([sy, cy])\n",
        "                mat_y = torch.stack([row0, row1]).to(torch.cfloat)\n",
        "\n",
        "                # Expand to full system\n",
        "                lst = [I] * self.n_qubits\n",
        "                lst[q] = mat_y\n",
        "                op = lst[0]\n",
        "                for k in range(1, self.n_qubits):\n",
        "                    op = torch.kron(op, lst[k])\n",
        "                psi = op @ psi\n",
        "\n",
        "                # --- APPLY RZ ---\n",
        "                ang_z = self.theta[d, q, 1]\n",
        "                val = ang_z / 2\n",
        "\n",
        "                # FIX: Construct complex exponentials carefully\n",
        "                # e^(-ix) = cos(x) - i*sin(x)\n",
        "                c_val = torch.cos(val)\n",
        "                s_val = torch.sin(val)\n",
        "\n",
        "                # Diagonal elements\n",
        "                elem0 = torch.complex(c_val, -s_val)\n",
        "                elem1 = torch.complex(c_val, s_val)\n",
        "                mat_z = torch.diag(torch.stack([elem0, elem1]))\n",
        "\n",
        "                lst = [I] * self.n_qubits\n",
        "                lst[q] = mat_z\n",
        "                op = lst[0]\n",
        "                for k in range(1, self.n_qubits):\n",
        "                    op = torch.kron(op, lst[k])\n",
        "                psi = op @ psi\n",
        "\n",
        "        return psi\n",
        "\n",
        "print(\"\\n--- PHASE 2: QuILT OPTIMIZATION ---\")\n",
        "\n",
        "# NOTE: If all costs are identical (e.g., -1.0 because training was too short on CPU),\n",
        "# we add tiny noise to valid_costs to demonstrate the VQE's ability to seek a minimum.\n",
        "# Otherwise VQE has no gradient to follow on a perfectly flat landscape.\n",
        "costs_tensor = torch.tensor(costs, dtype=torch.cfloat, device=device)\n",
        "if torch.std(costs_tensor.real) == 0:\n",
        "    print(\"Notice: Fitness landscape is flat (models need more training time). Injecting microspectrum noise for demo.\")\n",
        "    noise = torch.randn_like(costs_tensor.real) * 0.01\n",
        "    hamiltonian = torch.diag(costs_tensor + noise)\n",
        "else:\n",
        "    hamiltonian = torch.diag(costs_tensor)\n",
        "\n",
        "vqe = VQESelector(n_qubits_nas, depth=3).to(device)\n",
        "opt_vqe = torch.optim.Adam(vqe.parameters(), lr=0.1)\n",
        "\n",
        "print(\"Optimizing Quantum State...\")\n",
        "for epoch in range(101):\n",
        "    opt_vqe.zero_grad()\n",
        "    state = vqe()\n",
        "    # Minimize Expected Cost <psi|H|psi>\n",
        "    energy = torch.real(torch.vdot(state, hamiltonian @ state))\n",
        "    energy.backward()\n",
        "    opt_vqe.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"VQE Epoch {epoch}: System Energy = {energy.item():.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 4: RESULT\n",
        "# ==============================================================================\n",
        "\n",
        "final_state = vqe().detach()\n",
        "probs = (final_state.abs()**2).cpu().numpy()\n",
        "best_idx = np.argmax(probs)\n",
        "winner_config = configs[best_idx]\n",
        "winner_bitstring = bitstrings[best_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"  OPTIMAL ARCHITECTURE FOUND: {winner_config['name']}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Quantum Probability: {probs[best_idx]:.4f}\")\n",
        "print(f\"Configuration Specs:\")\n",
        "print(f\"   Layers:     {winner_config['n_layer']}\")\n",
        "print(f\"   Embed Dim:  {winner_config['n_embd']}\")\n",
        "print(f\"   Heads:      {winner_config['n_head']}\")\n",
        "print(f\"   Block Size: {winner_config['block_size']}\")\n",
        "print(f\"   LR:         {winner_config['lr']}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# ==============================================================================\n",
        "# PART 4: RESULT\n",
        "# ==============================================================================\n",
        "\n",
        "final_state = vqe().detach()\n",
        "probs = (final_state.abs()**2).cpu().numpy()\n",
        "best_idx = np.argmax(probs)\n",
        "winner_config = configs[best_idx]\n",
        "winner_bitstring = bitstrings[best_idx]\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"  OPTIMAL ARCHITECTURE FOUND: {winner_config['name']}\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Quantum Probability: {probs[best_idx]:.4f}\")\n",
        "print(f\"Configuration Specs:\")\n",
        "print(f\"   Layers:     {winner_config['n_layer']}\")\n",
        "print(f\"   Embed Dim:  {winner_config['n_embd']}\")\n",
        "print(f\"   Heads:      {winner_config['n_head']}\")\n",
        "print(f\"   Block Size: {winner_config['block_size']}\")\n",
        "print(f\"   LR:         {winner_config['lr']}\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Ready for Full Scale Training on chosen architecture.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "\n",
        "# ==============================================================================\n",
        "# CONFIGURATION: THE WINNER (Arch-1001)\n",
        "# ==============================================================================\n",
        "# Derived from your QuILT Output\n",
        "config = {\n",
        "    'name': 'Arch-1001 (QuILT Winner)',\n",
        "    'n_layer': 4,\n",
        "    'n_embd': 128,\n",
        "    'n_head': 8,\n",
        "    'block_size': 64,\n",
        "    'vocab_size': vocab_size, # Inherited from previous cell\n",
        "    'batch_size': 64,         # Increased for stability\n",
        "    'dropout': 0.1,\n",
        "    'lr': 0.0005              # The specific LR found by QuILT\n",
        "}\n",
        "\n",
        "print(f\"--- STARTING FULL SCALE TRAINING ---\")\n",
        "print(f\"Model: {config['name']}\")\n",
        "print(f\"Hyperparams: {config}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# MODEL SETUP (Re-instantiating the precise architecture)\n",
        "# ==============================================================================\n",
        "\n",
        "model = PicoTransformer(config).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "\n",
        "# Calculate Parameters\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Parameter Count: {n_params/1e6:.2f}M\")\n",
        "\n",
        "# ==============================================================================\n",
        "# TRAINING LOOP (Extended)\n",
        "# ==============================================================================\n",
        "\n",
        "max_iters = 3000  # Enough to learn basic arithmetic patterns\n",
        "eval_interval = 500\n",
        "\n",
        "start_time = time.time()\n",
        "losses = []\n",
        "\n",
        "model.train()\n",
        "for iter_num in range(max_iters):\n",
        "    # 1. Get Batch\n",
        "    xb, yb = get_batch('train', config['block_size'], config['batch_size'])\n",
        "\n",
        "    # 2. Forward / Backward\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 3. Logging\n",
        "    if iter_num % eval_interval == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Iter {iter_num}: Loss {loss.item():.4f} (Time: {elapsed:.1f}s)\")\n",
        "        losses.append(loss.item())\n",
        "\n",
        "print(f\"--- TRAINING COMPLETE in {time.time()-start_time:.1f}s ---\")\n",
        "\n",
        "# ==============================================================================\n",
        "# FINAL EXAM: TESTING MATHEMATICAL COMPETENCE\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n--- FINAL EXAM: DEEPMIND MATHEMATICS TEST ---\")\n",
        "model.eval()\n",
        "\n",
        "score = 0\n",
        "num_questions = 10\n",
        "end_token = stoi.get('\\n', None)\n",
        "\n",
        "for i in range(num_questions):\n",
        "    # Pick random test question\n",
        "    q, correct_a = random.choice(test_qa_pairs)\n",
        "\n",
        "    # Prompting\n",
        "    prompt = f\"{q}\\n\"\n",
        "    context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generation\n",
        "    try:\n",
        "        # Generate until newline or max tokens\n",
        "        out_ids = model.generate(context, max_new_tokens=15, end_token_id=end_token)[0].tolist()\n",
        "        # Decode only the new part\n",
        "        full_text = decode(out_ids)\n",
        "        model_answer = full_text[len(prompt):].strip()\n",
        "\n",
        "        # Grading\n",
        "        is_correct = (model_answer == correct_answer)\n",
        "        mark = \"✅\" if is_correct else \"❌\"\n",
        "        if is_correct: score += 1\n",
        "\n",
        "        print(f\"Q: {q}\")\n",
        "        print(f\"   Target: {correct_answer}\")\n",
        "        print(f\"   Model:  {model_answer} {mark}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating for: {q}\")\n",
        "\n",
        "print(f\"\\nFINAL SCORE: {score}/{num_questions} ({(score/num_questions)*100}%)\")\n",
        "\n",
        "if score > 0:\n",
        "    print(\"SUCCESS: The Quantum-Selected Architecture has learned to calculate!\")\n",
        "    print(\"This validates the entire QSFPGA -> QuILT -> Neural Network pipeline.\")\n",
        "else:\n",
        "    print(\"RESULT: Model needs more training time (increase max_iters to 10k+ for high accuracy).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6gP2zmvFuk-",
        "outputId": "f4b8d49c-3bca-4ba5-e57e-653a155034ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STARTING FULL SCALE TRAINING ---\n",
            "Model: Arch-1001 (QuILT Winner)\n",
            "Hyperparams: {'name': 'Arch-1001 (QuILT Winner)', 'n_layer': 4, 'n_embd': 128, 'n_head': 8, 'block_size': 64, 'vocab_size': 43, 'batch_size': 64, 'dropout': 0.1, 'lr': 0.0005}\n",
            "Parameter Count: 0.81M\n",
            "Iter 0: Loss 3.7768 (Time: 1.7s)\n",
            "Iter 500: Loss 1.2496 (Time: 417.2s)\n",
            "Iter 1000: Loss 1.1801 (Time: 830.8s)\n",
            "Iter 1500: Loss 1.1165 (Time: 1242.1s)\n",
            "Iter 2000: Loss 1.0565 (Time: 1656.7s)\n",
            "Iter 2500: Loss 1.0602 (Time: 2074.0s)\n",
            "--- TRAINING COMPLETE in 2485.9s ---\n",
            "\n",
            "--- FINAL EXAM: DEEPMIND MATHEMATICS TEST ---\n",
            "Error generating for: Add 10295.5 and 1.\n",
            "Error generating for: What is 3 less than -506?\n",
            "Error generating for: Total of 5.67 and -9.\n",
            "Error generating for: -2404 - -1.2\n",
            "Error generating for: Put together -0.5 and 192.\n",
            "Error generating for: Work out 54426 - 0.8.\n",
            "Error generating for: Work out -1.2 - 6.\n",
            "Error generating for: What is 0 + -171?\n",
            "Error generating for: What is the difference between -0.4 and 260.4?\n",
            "Error generating for: What is the difference between 33 and 18.6?\n",
            "\n",
            "FINAL SCORE: 0/10 (0.0%)\n",
            "RESULT: Model needs more training time (increase max_iters to 10k+ for high accuracy).\n"
          ]
        }
      ]
    }
  ]
}